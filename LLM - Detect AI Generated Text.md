# LLM - Detect AI Generated Text

*Competition Link: https://www.kaggle.com/competitions/llm-detect-ai-generated-text*

Reference Code: https://www.kaggle.com/code/awsaf49/detect-fake-text-kerasnlp-tf-torch-jax-train

My version: https://www.kaggle.com/code/wanyingxu2004/detect-fake-text-kerasnlp-tf-torch-jax-infer/edit



This notebook demonstrate the usage of the multiple-backend capabilities of **KerasCore** and **KerasNLP** for the *Detect Fake Text*.

## Install Libraries

```python
!pip install /kaggle/input/llm-science-exam-lib-ds/keras_core-0.1.7-py3-none-any.whl --no-deps
!pip install /kaggle/input/llm-science-exam-lib-ds/keras_nlp-0.6.2-py3-none-any.whl --no-deps
```



## Configuration

```python
class CFG:
    verbose = 0  # Verbosity
    device = 'GPU'  # Device
    seed = 42  # Random seed
    batch_size = 6  # Batch size
    drop_remainder = True  # Drop incomplete batches
    ckpt_dir = "/kaggle/input/daigt-kerasnlp-ckpt"  # Name of pretrained models
    sequence_length = 200  # Input sequence length
    class_names = ['real','fake']  # Class names [A, B, C, D, E]
    num_classes = len(class_names)  # Number of classes
    class_labels = list(range(num_classes))  # Class labels [0, 1, 2, 3, 4]
    label2name = dict(zip(class_labels, class_names))  # Label to class name mapping
    name2label = {v: k for k, v in label2name.items()}  # Class name to label mapping
```



## Library Version

```python
print("TensorFlow:", tf.__version__)
# print("JAX:", jax.__version__)
print("Keras:", keras.__version__)
print("KerasNLP:", keras_nlp.__version__)
```



## Configuration

```python
class CFG:
    verbose = 0  # Verbosity
    device = 'GPU'  # Device
    seed = 42  # Random seed
    batch_size = 6  # Batch size
    drop_remainder = True  # Drop incomplete batches
    ckpt_dir = "/kaggle/input/daigt-kerasnlp-ckpt"  # Name of pretrained models
    sequence_length = 200  # Input sequence length
    class_names = ['real','fake']  # Class names [A, B, C, D, E]
    num_classes = len(class_names)  # Number of classes
    class_labels = list(range(num_classes))  # Class labels [0, 1, 2, 3, 4Hi ]
    label2name = dict(zip(class_labels, class_names))  # Label to class name mapping
    name2label = {v: k for k, v in label2name.items()}  # Class name to label mapping
```



## Reproducibility

*Sets value for random seed to produce similar result in each run.*

```python
keras.utils.set_random_seed(CFG.seed)
```



## Hardware

Following codes automatically detects hardware (TPU or GPU)

```python
def get_device():
    "Detect and intializes GPU/TPU automatically"
    try:
        # Connect to TPU
        tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() 
        # Set TPU strategy
        strategy = tf.distribute.TPUStrategy(tpu)
        print(f'> Running on TPU', tpu.master(), end=' | ')
        print('Num of TPUs: ', strategy.num_replicas_in_sync)
        device=CFG.device
    except:
        # If TPU is not available, detect GPUs
        gpus = tf.config.list_logical_devices('GPU')
        ngpu = len(gpus)
         # Check number of GPUs
        if ngpu:
            # Set GPU strategy
            strategy = tf.distribute.MirroredStrategy(gpus) # single-GPU or multi-GPU
            # Print GPU details
            print("> Running on GPU", end=' | ')
            print("Num of GPUs: ", ngpu)
            device='GPU'
        else:
            # If no GPUs are available, use CPU
            print("> Running on CPU")
            strategy = tf.distribute.get_strategy()
            device='CPU'
    return strategy, device
```



```python
# Initialize GPU/TPU/TPU-VM
strategy, CFG.device = get_device()
CFG.replicas = strategy.num_replicas_in_sync
```



## Dataset Path

```python
BASE_PATH = '/kaggle/input/llm-detect-ai-generated-text'
```



## Meta Data

+ {test|train}_essays.csv
  + *id* - A unique identifier for each essay
  + *prompt_id* - Identifies the prompt the essay was written in response to.
  + *text* - the essay text itself.
  + *generated* - Whether the essay was written by a student (0) or generated by an LLM (1). This field is the target and is not present in test_essays_csv.
+ **sample_submission.csv** - is the valid sample submission



### Test Data

```python
test_df = pd.read_csv(f'{BASE_PATH}/test_essays.csv')  # Read CSV file into a DataFrame

# Display information about the train data
print("# Test Data: {:,}".format(len(test_df)))
print("# Sample:")
display(test_df.head(2))
```



## Preprocessing

**What is does**: The preprocessor takes input strings and transforms them into a dictionary (token_ids, padding_mask) containing preprocessed tensors. 

This process stats with tokenization, where input strings are converted into sequences of token IDs.

**Why it's important**: Initially, raw text data is complex and challenging for modeling due to it's high dimensionality. By converting text into a compact set of tokens, such as transforming *"The quick brown fox"* into *["the", "qu", "##ick", "br", "##own", "fox"]*, we simplify the data.  Many models rely on special tokens and additional tensors to understand input. These tokens help divide input and identify padding, among other tasks. Making all sequences the same length through padding boosts computational efficiency, making subsequent step smoother.

Explore the following pages to access the available preprocessing and tokenizer layers in **KerasNLP**.

- [Preprocessing](https://keras.io/api/keras_nlp/preprocessing_layers/)
- [Tokenizers](https://keras.io/api/keras_nlp/tokenizers/)

```python
vocab_path = '/kaggle/input/keras-nlp-deberta-v3-base-en-vocab-ds/vocab.spm'
tokenizer= keras_nlp.models.DebertaV3Tokenizer(vocab_path)
preprocessor= keras_nlp.models.DebertaV3Preprocessor(tokenizer, sequence_length=CFG.sequence_length)
```

Examine what the output shape of the preprocessing layer looks like.

The output shape of the layer can be represented as

*num_choices.sequence_length*

```python
uts = preprocessor(test_df.text.iloc[0])  # Process options for the first row

# Display the shape of each processed output
for k, v in outs.items():
    print(k, ":", v.shape)
```

We'll use the *preprocessing_fn* function to transform each text option using the *dataset.map(preprocessing_fn)* method.



